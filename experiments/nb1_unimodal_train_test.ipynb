{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and testing of unimodal speech and vision models\n",
    "\n",
    "**Author:** Ryan Eloff<br>\n",
    "**Contact:** ryan.peter.eloff@gmail.com<br>\n",
    "**Date:** October 2018\n",
    "\n",
    "Experiments notebook 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "We approach the *multimodal one-shot learning* problem by extending unimodal one-shot models to the multimodal case. \n",
    "Using speech and image background data (disjoint from the one-shot problem), we separately train unimodal speech and vision models that can perform one-shot classification in their respective modality.\n",
    "\n",
    "This notebook demonstrates how to train and test these unimodal one-shot models, and reproduces the one-shot speech (and image) classification results presented in [our paper](https://arxiv.org/abs/1811.03875): \n",
    "R. Eloff, H. A. Engelbrecht, H. Kamper, \"Multimodal One-Shot Learning of Speech and Images,\" 2018."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Navigation\n",
    "\n",
    "1. [Generate random model seeds](#seeds)<br>\n",
    "2. [Unimodal one-shot speech models](#speech)<br>\n",
    "    2.1. [Hyperparameters](#speech_params)<br>\n",
    "    2.2. [Background training](#speech_train)<br>\n",
    "    2.3. [One-shot speech testing](#speech_test)<br>\n",
    "    2.4. [Summaries](#speech_summ)<br>\n",
    "3. [Unimodal one-shot vision models](#vision)<br>\n",
    "    3.1. [Hyperparameters](#vision_params)<br>\n",
    "    3.2. [Background training](#vision_train)<br>\n",
    "    3.3. [One-shot vision testing](#vision_test)<br>\n",
    "    3.4. [Summaries](#vision_summ)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_speech_k_shot(model_dir, out_dir, random_seed, k_shot):\n",
    "    print(\"--------------------------------------------------------------------------------\")\n",
    "    print(\"Testing speech model: --model-dir={}\".format(model_dir))\n",
    "    print(\"--------------------------------------------------------------------------------\")\n",
    "    !python ../src/multimodal/test_speech.py \\\n",
    "        --data-dir=../kaldi_features/tidigits \\\n",
    "        --model-dir={model_dir} \\\n",
    "        --output-dir={out_dir} \\\n",
    "        --random-seed={random_seed} \\\n",
    "        --n-queries=10 \\\n",
    "        --n-test-episodes=400 \\\n",
    "        --k-shot={k_shot} \\\n",
    "        --l-way=11\n",
    "\n",
    "\n",
    "def test_speaker_invariance(model_dir, out_dir, random_seed):\n",
    "    print(\"--------------------------------------------------------------------------------\")\n",
    "    print(\"Testing speech model: --model-dir={}\".format(model_dir))\n",
    "    print(\"--------------------------------------------------------------------------------\")\n",
    "    !python ../src/multimodal/test_speech.py \\\n",
    "        --data-dir=../kaldi_features/tidigits \\\n",
    "        --model-dir={model_dir} \\\n",
    "        --output-dir={out_dir} \\\n",
    "        --random-seed={random_seed} \\\n",
    "        --originator-type='difficult' \\\n",
    "        --n-queries=1 \\\n",
    "        --n-test-episodes=4000 \\\n",
    "        --k-shot=1 \\\n",
    "        --l-way=11\n",
    "\n",
    "\n",
    "def test_vision_k_shot(model_dir, out_dir, random_seed, k_shot):\n",
    "    print(\"--------------------------------------------------------------------------------\")\n",
    "    print(\"Testing vision model: --model-dir={}\".format(model_dir))\n",
    "    print(\"--------------------------------------------------------------------------------\")\n",
    "    !python ../src/multimodal/test_vision.py \\\n",
    "        --model-dir={model_dir} \\\n",
    "        --output-dir={out_dir} \\\n",
    "        --random-seed={random_seed} \\\n",
    "        --n-queries=10 \\\n",
    "        --n-test-episodes=400 \\\n",
    "        --k-shot={k_shot} \\\n",
    "        --l-way=10\n",
    "\n",
    "\n",
    "def get_all_dirs(top_dir):\n",
    "    all_dirs = [os.path.join(top_dir, file)\n",
    "                  for file in os.listdir(top_dir)\n",
    "                  if os.path.isdir(os.path.join(top_dir, file))]\n",
    "    return all_dirs\n",
    "\n",
    "\n",
    "def summarise_tests(result_dir, result_file='test_result.txt', speaker_invariance=False):\n",
    "    result_dir = get_all_dirs(result_dir)\n",
    "    overall_results = []\n",
    "    easy_overall_results = []\n",
    "    dist_overall_results = []\n",
    "    for root, subdirs, files in os.walk(result_dir):\n",
    "        subdirs.sort()\n",
    "        for dirname in subdirs:\n",
    "            res_file = os.path.join(root, dirname, result_file)\n",
    "            if os.path.isfile(res_file):\n",
    "                print(\"--------------------------------------------------------------------------------\")\n",
    "                print(\"Model summary: directory={}\".format(os.path.join(root, dirname)))\n",
    "                print(\"--------------------------------------------------------------------------------\")\n",
    "                with open(res_file, 'r') as fp:\n",
    "                    results = fp.read()\n",
    "                print('\\tResults: {}'.format(results))\n",
    "                overall_results.append(float(results.split('\\n')[0].split('accuracy: ')[1]))\n",
    "                if speaker_invariance:\n",
    "                    invariance_results = results.split('\\n')[1].strip().split('\\t')\n",
    "                    easy_overall_results.append(float(invariance_results[0].split('accuracy: ')[1]))\n",
    "                    dist_overall_results.append(float(invariance_results[1].split('accuracy: ')[1]))\n",
    "    conf_interval_95 = 1.96 * np.std(overall_results) / np.sqrt(len(overall_results))\n",
    "    easy_conf_interval_95 = 1.96 * np.std(easy_overall_results) / np.sqrt(len(easy_overall_results))\n",
    "    dist_conf_interval_95 = 1.96 * np.std(dist_overall_results) / np.sqrt(len(dist_overall_results))\n",
    "    print(\"================================================================================\")\n",
    "    print(\"OVERALL: AVERAGE ACCURACY: {:.4f} % +- {:.4f} (total tests: {})\"\n",
    "          .format(np.mean(overall_results)*100, conf_interval_95*100, len(overall_results)))\n",
    "    if speaker_invariance:\n",
    "        print(\"\\tAVERAGE EASY SPEAKER ACCURACY: {:.4f} % +- {:.4f} (total tests: {})\"\n",
    "              .format(np.mean(easy_overall_results)*100, easy_conf_interval_95*100, len(easy_overall_results)))\n",
    "        print(\"\\tAVERAGE DISTRACTOR SPEAKER ACCURACY: {:.4f} % +- {:.4f} (total tests: {})\"\n",
    "              .format(np.mean(dist_overall_results)*100, dist_conf_interval_95*100, len(dist_overall_results)))\n",
    "    print(\"--------------------------------------------------------------------------------\")\n",
    "\n",
    "    \n",
    "def summarise_train_time(result_dir, log_file='train_speech.log'):\n",
    "    result_dir = get_all_dirs(result_dir)\n",
    "    all_results = []\n",
    "    n_train_results = 0\n",
    "    for root, subdirs, files in os.walk(result_dir):\n",
    "        subdirs.sort()\n",
    "        for dirname in subdirs:\n",
    "            next_file = os.path.join(root, dirname, log_file)\n",
    "            if os.path.isfile(next_file):\n",
    "                with open(next_file, 'r') as fp:\n",
    "                    log_lines = fp.readlines()\n",
    "                    start_time = datetime.datetime.strptime(log_lines[0].split(\":INFO:\")[0], \"%Y-%m-%d %H:%M:%S,%f\")\n",
    "                    end_time = datetime.datetime.strptime(log_lines[-1].split(\":INFO:\")[0], \"%Y-%m-%d %H:%M:%S,%f\")\n",
    "                    train_minutes = (end_time - start_time).total_seconds()/60.0\n",
    "                    print(end_time - start_time)\n",
    "                    all_results.append(train_minutes)\n",
    "    total_results = all_results[0]\n",
    "    for i in range(1, len(all_results)):\n",
    "        total_results += all_results[i]\n",
    "    conf_interval_95 = 1.96 * np.std(all_results) / np.sqrt(len(all_results))\n",
    "    print(\"Average train duration: {:.4f} min. +- {:.4f}\".format(np.mean(all_results), conf_interval_95))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate random model seeds\n",
    "<a id='seeds'></a>\n",
    "\n",
    "We average results over 10 models trained with different seeds so that we can report average accuracies with 95% confidence intervals.\n",
    "\n",
    "These seeds are generated as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "random_seeds = np.random.randint(1000, size=10)\n",
    "print(\"Random seeds:\", random_seeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Unimodal one-shot speech models\n",
    "<a id='speech'></a>\n",
    "\n",
    "Several different models can be used for one-shot speech classification.\n",
    "We specifically investigate Siamese neural networks (and their advances) as a way to explicitly train unimodal distance metrics.\n",
    "We also compare to directly using extracted speech features with classical dynamic time warping (DTW) and to transfer learning with neural network classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Hyperparameters\n",
    "<a id='speech_params'></a>\n",
    "\n",
    "The following hyperparameters were used to produce the (speech) results in the paper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training options:\n",
    "learn_rate = 1e-3\n",
    "decay_rate = 0.96\n",
    "\n",
    "# Network architectures:\n",
    "kernel_sizes = [[39, 9], [1, 10]]\n",
    "pool_sizes = [[1, 3], [1, 28]]\n",
    "n_filters = [128, 128]\n",
    "n_hidden_units = [2048]\n",
    "n_linear_units = None\n",
    "ffnn_n_hidden_units = [512, 512, 512]\n",
    "\n",
    "# Regularization:\n",
    "dropout_keep_prob = 0.9\n",
    "dropout_channels = True\n",
    "conv_batch_norm = 'after'\n",
    "hidden_batch_norm = 'after'\n",
    "\n",
    "# Batching:\n",
    "balanced_batching = True\n",
    "p_batch = 128  # 32 for Siamese CNN (offline)\n",
    "k_batch = 8  # 2 for Siamese CNN (offline)\n",
    "ffnn_batch_size = 200\n",
    "cnn_batch_size = 200\n",
    "\n",
    "# Feature selection:\n",
    "feats_type = 'mfcc'\n",
    "center_padded = True\n",
    "n_padded = 120\n",
    "\n",
    "# Early stopping on one-shot validation error:\n",
    "l_way_validation = 20\n",
    "\n",
    "# Siamese CNN parameters:\n",
    "triplet_margin = 0.2\n",
    "# Train episodes based on ~88400 examples and ~5500 classes (digits removed)\n",
    "# - Siamese CNN (offline): choose 5500/32 ~ 200 (each epoch processes 200*32*2=12800 examples)\n",
    "# - Siamese CNN (online): choose 5500/128 ~ 50 (each epoch processes 50*128*8=51200 examples)\n",
    "# NOTE: offline variant takes long to train; online is more efficient and reaches SOTA in fewer epochs\n",
    "offline_n_train_episodes = 200\n",
    "online_n_train_episodes = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Background training\n",
    "<a id='speech_train'></a>\n",
    "\n",
    "Neural network models are trained on a large set of isolated words from background speech data ([Flickr 8k audio caption corpus](https://groups.csail.mit.edu/sls/downloads/flickraudio/); all overlapping one-shot test words removed), learning features which are useful for measuring similarity between inputs (even on unseen classes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic Time Warping (DTW)\n",
    "\n",
    "No training for DTW, just create directories containing model options for test script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for dtw_feats_type in ['mfcc', 'fbank']:\n",
    "    for random_seed in random_seeds:\n",
    "        model_dir = os.path.join(\"./models/speech/dtw\", dtw_feats_type, \"random_seed={}\".format(random_seed))\n",
    "        print(\"-----------------------------------------------------------------------------------\")\n",
    "        print(\"Create DTW speech model params: --random-seed={} --feats-type={}\".format(random_seed, dtw_feats_type))\n",
    "        print(\"-----------------------------------------------------------------------------------\")\n",
    "        # Create DTW model parameters\n",
    "        !python ../src/multimodal/train_speech.py \\\n",
    "            --model-version=dtw \\\n",
    "            --data-dir=../kaldi_features/flickr_audio \\\n",
    "            --model-dir={model_dir}\\\n",
    "            --no-unique-dir \\\n",
    "            --n-max-epochs=100 \\\n",
    "            --random-seed={random_seed} \\\n",
    "            --feats-type={dtw_feats_type}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedforward Neural Network (FFNN) Softmax Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for random_seed in random_seeds:\n",
    "    model_dir = \"./models/speech/ffnn_softmax/batch_size={}/random_seed={}\".format(ffnn_batch_size,\n",
    "                                                                                   random_seed)\n",
    "    print(\"-----------------------------------------------------------------------------------\")\n",
    "    print(\"Training FFNN classifier speech model: --random-seed={}\".format(random_seed))\n",
    "    print(\"-----------------------------------------------------------------------------------\")\n",
    "    # Save base params file in model directory\n",
    "    !python ../src/multimodal/train_speech.py \\\n",
    "        --model-version=feedforward_softmax \\\n",
    "        --model-dir={model_dir} \\\n",
    "        --no-unique-dir \\\n",
    "        --save-base-params\n",
    "    # Load model params from file\n",
    "    with open(os.path.join(model_dir, 'model_params.json'), 'r') as fp:\n",
    "        model_params = json.load(fp)\n",
    "    # Set new model params\n",
    "    model_params['n_hidden_units'] = ffnn_n_hidden_units\n",
    "    model_params['n_linear_units'] = None  # automatically adds output logits\n",
    "    model_params['dropout_keep_prob'] = dropout_keep_prob\n",
    "    model_params['hidden_batch_norm'] = hidden_batch_norm\n",
    "    # Save updated model params to file\n",
    "    with open(os.path.join(model_dir, 'model_params.json'), 'w') as fp:\n",
    "        json.dump(model_params, fp, indent=4)\n",
    "    # Train ffnn clasifier speech model on updated model params\n",
    "    !python ../src/multimodal/train_speech.py \\\n",
    "        --model-version=feedforward_softmax \\\n",
    "        --data-dir=../kaldi_features/flickr_audio \\\n",
    "        --model-dir={model_dir}\\\n",
    "        --no-unique-dir \\\n",
    "        --n-max-epochs=100 \\\n",
    "        --random-seed={random_seed} \\\n",
    "        -lr={learn_rate} \\\n",
    "        -dr={decay_rate} \\\n",
    "        --batch-size={ffnn_batch_size} \\\n",
    "        --center-padded \\\n",
    "        --n-padded={n_padded} \\\n",
    "        --feats-type={feats_type} \\\n",
    "        --k-shot=1 \\\n",
    "        --l-way={l_way_validation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network (CNN) Softmax Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for random_seed in random_seeds:\n",
    "    model_dir = \"./models/speech/cnn_softmax/batch_size={}/random_seed={}\".format(cnn_batch_size,\n",
    "                                                                                  random_seed)\n",
    "    print(\"-----------------------------------------------------------------------------------\")\n",
    "    print(\"Training CNN classifier speech model: --random-seed={}\".format(random_seed))\n",
    "    print(\"-----------------------------------------------------------------------------------\")\n",
    "    # Save base params file in model directory\n",
    "    !python ../src/multimodal/train_speech.py \\\n",
    "        --model-version=convolutional_softmax \\\n",
    "        --model-dir={model_dir} \\\n",
    "        --no-unique-dir \\\n",
    "        --save-base-params\n",
    "    # Load model params from file\n",
    "    with open(os.path.join(model_dir, 'model_params.json'), 'r') as fp:\n",
    "        model_params = json.load(fp)\n",
    "    # Set new model params\n",
    "    model_params['kernel_sizes'] = kernel_sizes\n",
    "    model_params['pool_sizes'] = pool_sizes\n",
    "    model_params['n_filters'] = n_filters\n",
    "    model_params['n_hidden_units'] = n_hidden_units\n",
    "    model_params['n_linear_units'] = None  # automatically adds output logits\n",
    "    model_params['dropout_keep_prob'] = dropout_keep_prob\n",
    "    model_params['dropout_channels'] = dropout_channels\n",
    "    model_params['conv_batch_norm'] = conv_batch_norm\n",
    "    model_params['hidden_batch_norm'] = hidden_batch_norm\n",
    "    # Save updated model params to file\n",
    "    with open(os.path.join(model_dir, 'model_params.json'), 'w') as fp:\n",
    "        json.dump(model_params, fp, indent=4)\n",
    "    # Train cnn clasifier speech model on updated model params\n",
    "    !python ../src/multimodal/train_speech.py \\\n",
    "        --model-version=convolutional_softmax \\\n",
    "        --data-dir=../kaldi_features/flickr_audio \\\n",
    "        --model-dir={model_dir}\\\n",
    "        --no-unique-dir \\\n",
    "        --n-max-epochs=100 \\\n",
    "        --random-seed={random_seed} \\\n",
    "        -lr={learn_rate} \\\n",
    "        -dr={decay_rate} \\\n",
    "        --batch-size={cnn_batch_size} \\\n",
    "        --center-padded \\\n",
    "        --n-padded={n_padded} \\\n",
    "        --feats-type={feats_type} \\\n",
    "        --k-shot=1 \\\n",
    "        --l-way={l_way_validation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Siamese CNN (offline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust k_batch and p_batch since Siamese offline variant hits memory limit\n",
    "# on a single Titan Xp GPU with larger batch sizes ...\n",
    "p_batch_adjust = 32  # wide spread of concepts per batch\n",
    "k_batch_adjust = 2   # few concept examples per batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for random_seed in random_seeds:\n",
    "    model_dir = \"./models/speech/siamese_offline/n_train={}/random_seed={}\".format(offline_n_train_episodes,\n",
    "                                                                                   random_seed)\n",
    "    print(\"-----------------------------------------------------------------------------------\")\n",
    "    print(\"Training Siamese CNN (offline) speech model: --random-seed={}\".format(random_seed))\n",
    "    print(\"-----------------------------------------------------------------------------------\")\n",
    "    # Save base params file in model directory\n",
    "    !python ../src/multimodal/train_speech.py \\\n",
    "        --model-version=siamese_triplet \\\n",
    "        --model-dir={model_dir} \\\n",
    "        --no-unique-dir \\\n",
    "        --save-base-params\n",
    "    # Load model params from file\n",
    "    with open(os.path.join(model_dir, 'model_params.json'), 'r') as fp:\n",
    "        model_params = json.load(fp)\n",
    "    # Set new model params\n",
    "    model_params['kernel_sizes'] = kernel_sizes\n",
    "    model_params['pool_sizes'] = pool_sizes\n",
    "    model_params['n_filters'] = n_filters\n",
    "    model_params['n_hidden_units'] = n_hidden_units\n",
    "    model_params['n_linear_units'] = n_linear_units\n",
    "    model_params['dropout_keep_prob'] = dropout_keep_prob\n",
    "    model_params['dropout_channels'] = dropout_channels\n",
    "    model_params['conv_batch_norm'] = conv_batch_norm\n",
    "    model_params['hidden_batch_norm'] = hidden_batch_norm\n",
    "    model_params['triplet_margin'] = triplet_margin\n",
    "    # Save updated model params to file\n",
    "    with open(os.path.join(model_dir, 'model_params.json'), 'w') as fp:\n",
    "        json.dump(model_params, fp, indent=4)\n",
    "    # Train siamese offline speech model on updated model params\n",
    "    !python ../src/multimodal/train_speech.py \\\n",
    "        --model-version=siamese_triplet \\\n",
    "        --data-dir=../kaldi_features/flickr_audio \\\n",
    "        --model-dir={model_dir}\\\n",
    "        --no-unique-dir \\\n",
    "        --n-max-epochs=100 \\\n",
    "        --n-train-episodes={offline_n_train_episodes} \\\n",
    "        --random-seed={random_seed} \\\n",
    "        -lr={learn_rate} \\\n",
    "        -dr={decay_rate} \\\n",
    "        --balanced-batching \\\n",
    "        --p-batch={p_batch_adjust} \\\n",
    "        --k-batch={k_batch_adjust} \\\n",
    "        --center-padded \\\n",
    "        --n-padded={n_padded} \\\n",
    "        --feats-type={feats_type} \\\n",
    "        --k-shot=1 \\\n",
    "        --l-way={l_way_validation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Siamese CNN (online)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for random_seed in random_seeds:\n",
    "    model_dir = \"./models/speech/siamese_online/n_train={}/random_seed={}\".format(online_n_train_episodes,\n",
    "                                                                                   random_seed)\n",
    "    print(\"-----------------------------------------------------------------------------------\")\n",
    "    print(\"Training Siamese CNN (online) speech model: --random-seed={}\".format(random_seed))\n",
    "    print(\"-----------------------------------------------------------------------------------\")\n",
    "    # Save base params file in model directory\n",
    "    !python ../src/multimodal/train_speech.py \\\n",
    "        --model-version=siamese_triplet_online \\\n",
    "        --model-dir={model_dir} \\\n",
    "        --no-unique-dir \\\n",
    "        --save-base-params\n",
    "    # Load model params from file\n",
    "    with open(os.path.join(model_dir, 'model_params.json'), 'r') as fp:\n",
    "        model_params = json.load(fp)\n",
    "    # Set new model params\n",
    "    model_params['kernel_sizes'] = kernel_sizes\n",
    "    model_params['pool_sizes'] = pool_sizes\n",
    "    model_params['n_filters'] = n_filters\n",
    "    model_params['n_hidden_units'] = n_hidden_units\n",
    "    model_params['n_linear_units'] = n_linear_units\n",
    "    model_params['dropout_keep_prob'] = dropout_keep_prob\n",
    "    model_params['dropout_channels'] = dropout_channels\n",
    "    model_params['conv_batch_norm'] = conv_batch_norm\n",
    "    model_params['hidden_batch_norm'] = hidden_batch_norm\n",
    "    model_params['triplet_margin'] = triplet_margin\n",
    "    # Save updated model params to file\n",
    "    with open(os.path.join(model_dir, 'model_params.json'), 'w') as fp:\n",
    "        json.dump(model_params, fp, indent=4)\n",
    "    # Train siamese online speech model on updated model params\n",
    "    !python ../src/multimodal/train_speech.py \\\n",
    "        --model-version=siamese_triplet_online \\\n",
    "        --data-dir=../kaldi_features/flickr_audio \\\n",
    "        --model-dir={model_dir}\\\n",
    "        --no-unique-dir \\\n",
    "        --n-max-epochs=100 \\\n",
    "        --n-train-episodes={online_n_train_episodes} \\\n",
    "        --random-seed={random_seed} \\\n",
    "        -lr={learn_rate} \\\n",
    "        -dr={decay_rate} \\\n",
    "        --balanced-batching \\\n",
    "        --p-batch={p_batch} \\\n",
    "        --k-batch={k_batch} \\\n",
    "        --center-padded \\\n",
    "        --n-padded={n_padded} \\\n",
    "        --feats-type={feats_type} \\\n",
    "        --k-shot=1 \\\n",
    "        --l-way={l_way_validation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. One-shot speech testing\n",
    "<a id='speech_test'></a>\n",
    "\n",
    "We now test the trained unimodal speech models on three tasks on the [TIDigits speech corpus](https://catalog.ldc.upenn.edu/LDC93S10):\n",
    "\n",
    "1. One-shot 11-way spoken digit classification\n",
    "2. Five-shot 11-way spoken digit classification\n",
    "3. Speaker invariance for one-shot 11-way spoken digit classification in the presence of query speaker distractors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic Time Warping (DTW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. One-shot 11-way spoken digit classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./results/speech/dtw/1_shot\"\n",
    "for dtw_feats_type in ['mfcc', 'fbank']:\n",
    "    for random_seed in random_seeds:\n",
    "        model_dir = os.path.join(\"./models/speech/dtw\", dtw_feats_type, \"random_seed={}\".format(random_seed))\n",
    "        out_dir = os.path.join(output_dir, dtw_feats_type, 'random_seed={}'.format(random_seed))\n",
    "        test_speech_k_shot(model_dir, out_dir, random_seed, k_shot=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Five-shot 11-way spoken digit classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./results/speech/dtw/5_shot\"\n",
    "for dtw_feats_type in ['mfcc', 'fbank']:\n",
    "    for random_seed in random_seeds:\n",
    "        model_dir = os.path.join(\"./models/speech/dtw\", dtw_feats_type, \"random_seed={}\".format(random_seed))\n",
    "        out_dir = os.path.join(output_dir, dtw_feats_type, 'random_seed={}'.format(random_seed))\n",
    "        test_speech_k_shot(model_dir, out_dir, random_seed, k_shot=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Speaker invariance for one-shot 11-way spoken digit classification in the presence of query speaker distractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./results/speech/dtw/speaker_invariance\"\n",
    "for dtw_feats_type in ['mfcc', 'fbank']:\n",
    "    for random_seed in random_seeds:\n",
    "        model_dir = os.path.join(\"./models/speech/dtw\", dtw_feats_type, \"random_seed={}\".format(random_seed))\n",
    "        out_dir = os.path.join(output_dir, dtw_feats_type, 'random_seed={}'.format(random_seed))\n",
    "        test_speaker_invariance(model_dir, out_dir, random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedforward Neural Network (FFNN) Softmax Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. One-shot 11-way spoken digit classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./results/speech/ffnn_softmax/1_shot/batch_size={}/\".format(ffnn_batch_size)\n",
    "for random_seed in random_seeds:\n",
    "    model_dir = \"./models/speech/ffnn_softmax/batch_size={}/random_seed={}\".format(ffnn_batch_size,\n",
    "                                                                                   random_seed)\n",
    "    out_dir = os.path.join(output_dir, 'random_seed={}'.format(random_seed))\n",
    "    test_speech_k_shot(model_dir, out_dir, random_seed, k_shot=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Five-shot 11-way spoken digit classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./results/speech/ffnn_softmax/5_shot/batch_size={}/\".format(ffnn_batch_size)\n",
    "for random_seed in random_seeds:\n",
    "    model_dir = \"./models/speech/ffnn_softmax/batch_size={}/random_seed={}\".format(ffnn_batch_size,\n",
    "                                                                                   random_seed)\n",
    "    out_dir = os.path.join(output_dir, 'random_seed={}'.format(random_seed))\n",
    "    test_speech_k_shot(model_dir, out_dir, random_seed, k_shot=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Speaker invariance for one-shot 11-way spoken digit classification in the presence of query speaker distractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./results/speech/ffnn_softmax/speaker_invariance/batch_size={}\".format(ffnn_batch_size)\n",
    "for random_seed in random_seeds:\n",
    "    model_dir = \"./models/speech/ffnn_softmax/batch_size={}/random_seed={}\".format(ffnn_batch_size,\n",
    "                                                                                   random_seed)\n",
    "    out_dir = os.path.join(output_dir, 'random_seed={}'.format(random_seed))\n",
    "    test_speaker_invariance(model_dir, out_dir, random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network (CNN) Softmax Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. One-shot 11-way spoken digit classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./results/speech/cnn_softmax/1_shot/batch_size={}\".format(cnn_batch_size)\n",
    "for random_seed in random_seeds:\n",
    "    model_dir = \"./models/speech/cnn_softmax/batch_size={}/random_seed={}\".format(cnn_batch_size, random_seed)\n",
    "    out_dir = os.path.join(output_dir, 'random_seed={}'.format(random_seed))\n",
    "    test_speech_k_shot(model_dir, out_dir, random_seed, k_shot=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Five-shot 11-way spoken digit classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./results/speech/cnn_softmax/5_shot/batch_size={}\".format(batch_size_adjust)\n",
    "for random_seed in random_seeds:\n",
    "    model_dir = \"./models/speech/cnn_softmax/batch_size={}/random_seed={}\".format(cnn_batch_size, random_seed)\n",
    "    out_dir = os.path.join(output_dir, 'random_seed={}'.format(random_seed))\n",
    "    test_speech_k_shot(model_dir, out_dir, random_seed, k_shot=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Speaker invariance for one-shot 11-way spoken digit classification in the presence of query speaker distractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./results/speech/cnn_softmax/speaker_invariance/batch_size={}\".format(cnn_batch_size)\n",
    "for random_seed in random_seeds:\n",
    "    model_dir = \"./models/speech/cnn_softmax/batch_size={}/random_seed={}\".format(cnn_batch_size, random_seed)\n",
    "    out_dir = os.path.join(output_dir, 'random_seed={}'.format(random_seed))\n",
    "    test_speaker_invariance(model_dir, out_dir, random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Siamese CNN (offline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. One-shot 11-way spoken digit classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./results/speech/siamese_offline/1_shot/n_train={}\".format(offline_n_train_episodes)\n",
    "for random_seed in random_seeds:\n",
    "    model_dir = \"./models/speech/siamese_offline/n_train={}/random_seed={}\".format(offline_n_train_episodes,\n",
    "                                                                                   random_seed)\n",
    "    out_dir = os.path.join(output_dir, 'random_seed={}'.format(random_seed))\n",
    "    test_speech_k_shot(model_dir, out_dir, random_seed, k_shot=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Five-shot 11-way spoken digit classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./results/speech/siamese_offline/5_shot/n_train={}\".format(offline_n_train_episodes)\n",
    "for random_seed in random_seeds:\n",
    "    model_dir = \"./models/speech/siamese_offline/n_train={}/random_seed={}\".format(offline_n_train_episodes,\n",
    "                                                                                   random_seed)\n",
    "    out_dir = os.path.join(output_dir, 'random_seed={}'.format(random_seed))\n",
    "    test_speech_k_shot(model_dir, out_dir, random_seed, k_shot=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Speaker invariance for one-shot 11-way spoken digit classification in the presence of query speaker distractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./results/speech/siamese_offline/speaker_invariance/n_train={}\".format(offline_n_train_episodes)\n",
    "for random_seed in random_seeds:\n",
    "    model_dir = \"./models/speech/siamese_offline/n_train={}/random_seed={}\".format(offline_n_train_episodes,\n",
    "                                                                                   random_seed)\n",
    "    out_dir = os.path.join(output_dir, 'random_seed={}'.format(random_seed))\n",
    "    test_speaker_invariance(model_dir, out_dir, random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Siamese CNN (online)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. One-shot 11-way spoken digit classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./results/speech/siamese_online/1_shot/n_train={}\".format(online_n_train_episodes)\n",
    "for random_seed in random_seeds:\n",
    "    model_dir = \"./models/speech/siamese_online/n_train={}/random_seed={}\".format(online_n_train_episodes,\n",
    "                                                                                  random_seed)\n",
    "    out_dir = os.path.join(output_dir, 'random_seed={}'.format(random_seed))\n",
    "    test_speech_k_shot(model_dir, out_dir, random_seed, k_shot=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Five-shot 11-way spoken digit classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./results/speech/siamese_online/5_shot/n_train={}\".format(online_n_train_episodes)\n",
    "for random_seed in random_seeds:\n",
    "    model_dir = \"./models/speech/siamese_online/n_train={}/random_seed={}\".format(online_n_train_episodes,\n",
    "                                                                                  random_seed)\n",
    "    out_dir = os.path.join(output_dir, 'random_seed={}'.format(random_seed))\n",
    "    test_speech_k_shot(model_dir, out_dir, random_seed, k_shot=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Speaker invariance for one-shot 11-way spoken digit classification in the presence of query speaker distractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./results/speech/siamese_online/speaker_invariance/n_train={}\".format(online_n_train_episodes)\n",
    "for random_seed in random_seeds:\n",
    "    model_dir = \"./models/speech/siamese_online/n_train={}/random_seed={}\".format(online_n_train_episodes,\n",
    "                                                                                  random_seed)\n",
    "    out_dir = os.path.join(output_dir, 'random_seed={}'.format(random_seed))\n",
    "    test_speaker_invariance(model_dir, out_dir, random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Summaries\n",
    "<a id='speech_summ'></a>\n",
    "\n",
    "This section presents summaries on the training and one-shot testing of the unimodal speech models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic Time Warping (DTW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. One-shot 11-way spoken digit classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir = os.path.join('./results/speech/dtw/1_shot', feats_type)\n",
    "summarise_tests(result_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Five-shot 11-way spoken digit classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir = os.path.join('./results/speech/dtw/5_shot', feats_type)\n",
    "summarise_tests(result_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Speaker invariance for one-shot 11-way spoken digit classification in the presence of query speaker distractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir = os.path.join('./results/speech/dtw/speaker_invariance', feats_type)\n",
    "summarise_tests(result_dir, speaker_invariance=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedforward Neural Network (FFNN) Softmax Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir = \"./models/speech/ffnn_softmax/batch_size={}\".format(ffnn_batch_size)\n",
    "summarise_tests(result_dir, result_file='train_result.txt', result_file='train_result.txt')\n",
    "summarise_train_time(result_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. One-shot 11-way spoken digit classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir = \"./results/speech/ffnn_softmax/1_shot/batch_size={}\".format(ffnn_batch_size)\n",
    "summarise_tests(result_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Five-shot 11-way spoken digit classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir = \"./results/speech/ffnn_softmax/5_shot/batch_size={}\".format(ffnn_batch_size)\n",
    "summarise_tests(result_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Speaker invariance for one-shot 11-way spoken digit classification in the presence of query speaker distractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir = \"./results/speech/ffnn_softmax/speaker_invariance/batch_size={}\".format(ffnn_batch_size)\n",
    "summarise_tests(result_dir, speaker_invariance=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network (CNN) Softmax Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir = \"./models/speech/cnn_softmax/batch_size={}\".format(cnn_batch_size)\n",
    "summarise_tests(result_dir, result_file='train_result.txt', result_file='train_result.txt')\n",
    "summarise_train_time(result_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. One-shot 11-way spoken digit classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir = \"./results/speech/cnn_softmax/1_shot/batch_size={}\".format(cnn_batch_size)\n",
    "summarise_tests(result_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Five-shot 11-way spoken digit classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir = \"./results/speech/cnn_softmax/5_shot/batch_size={}\".format(cnn_batch_size)\n",
    "summarise_tests(result_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Speaker invariance for one-shot 11-way spoken digit classification in the presence of query speaker distractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir = \"./results/speech/cnn_softmax/speaker_invariance/batch_size={}\".format(cnn_batch_size)\n",
    "summarise_tests(result_dir, speaker_invariance=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Siamese CNN (offline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir = \"./models/speech/siamese_offline/n_train={}\".format(offline_n_train_episodes)\n",
    "summarise_tests(result_dir, result_file='train_result.txt')\n",
    "summarise_train_time(result_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. One-shot 11-way spoken digit classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir = \"./results/speech/siamese_offline/1_shot/n_train={}\".format(offline_n_train_episodes)\n",
    "summarise_tests(result_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Five-shot 11-way spoken digit classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir = \"./results/speech/siamese_offline/5_shot/n_train={}\".format(offline_n_train_episodes)\n",
    "summarise_tests(result_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Speaker invariance for one-shot 11-way spoken digit classification in the presence of query speaker distractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir = \"./results/speech/siamese_offline/speaker_invariance/n_train={}\".format(offline_n_train_episodes)\n",
    "summarise_tests(result_dir, speaker_invariance=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Siamese CNN (online)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir = \"./models/speech/siamese_online/n_train={}\".format(online_n_train_episodes)\n",
    "summarise_tests(result_dir, result_file='train_result.txt')\n",
    "summarise_train_time(result_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. One-shot 11-way spoken digit classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir = \"./results/speech/siamese_online/1_shot/n_train={}\".format(online_n_train_episodes)\n",
    "summarise_tests(result_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Five-shot 11-way spoken digit classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir = \"./results/speech/siamese_online/5_shot/n_train={}\".format(online_n_train_episodes)\n",
    "summarise_tests(result_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Speaker invariance for one-shot 11-way spoken digit classification in the presence of query speaker distractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir = \"./results/speech/siamese_online/speaker_invariance/n_train={}\".format(online_n_train_episodes)\n",
    "summarise_tests(result_dir, speaker_invariance=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Unimodal one-shot vision models\n",
    "<a id='vision'></a>\n",
    "\n",
    "Similar to the unimodal speech case, we investigate Siamese neural networks for one-shot image classification,\n",
    "and compare to directly matching image pixels with cosine similarity, and to transfer learning with neural network classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Hyperparameters\n",
    "<a id='vision_params'></a>\n",
    "\n",
    "The following hyperparameters were used to produce the (vision) results in the paper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training options:\n",
    "learn_rate = 1e-3\n",
    "decay_rate = 0.96\n",
    "\n",
    "# Network architectures:\n",
    "kernel_sizes = [[3, 3], [3, 3], [3, 3]]\n",
    "pool_sizes = [[2, 2], [2, 2], None]\n",
    "n_filters = [32, 64, 128]\n",
    "n_hidden_units = [2048]\n",
    "n_linear_units = 1024\n",
    "ffnn_n_hidden_units = [512, 512, 512]\n",
    "\n",
    "# Regularization: \n",
    "dropout_keep_prob = 0.9  # 0.7\n",
    "dropout_channels = True\n",
    "conv_batch_norm = 'before'  # 'after'\n",
    "hidden_batch_norm = 'before'  # 'after'\n",
    "\n",
    "# Batching:\n",
    "balanced_batching = True\n",
    "p_batch = 128  # 32 for Siamese CNN (offline)\n",
    "k_batch = 8  # 2 for Siamese CNN (offline)\n",
    "ffnn_batch_size = 200\n",
    "cnn_batch_size = 200\n",
    "\n",
    "# Early stopping on one-shot validation error:\n",
    "l_way_validation = 20\n",
    "\n",
    "# Siamese CNN parameters:\n",
    "triplet_margin = 0.2\n",
    "# Train episodes based on 19200 examples and 964 classes\n",
    "# - Siamese CNN (offline): choose 19200/32 = 600 (each epoch processes 600*32*2=38400 examples)\n",
    "# - Siamese CNN (online): choose 19200/128 = 150 (each epoch processes 150*128*8=153600 examples)\n",
    "# NOTE: offline variant takes long to train; online is more efficient and reaches SOTA in fewer epochs\n",
    "offline_n_train_episodes = 600\n",
    "online_n_train_episodes = 150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Background training\n",
    "<a id='vision_train'></a>\n",
    "\n",
    "Once again, neural network models are trained on a large set of images from background image data ([Omniglot handwritten characters](https://github.com/brendenlake/omniglot/); no overlap with one-shot test images), learning features which are useful for measuring similarity between inputs (even on unseen classes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pixel Matching\n",
    "\n",
    "No training, just create directories containing model options for test script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for random_seed in random_seeds:\n",
    "    model_dir = \"./models/vision/pixels/random_seed={}\".format(random_seed)\n",
    "    print(\"-----------------------------------------------------------------------------------\")\n",
    "    print(\"Create pixel matching vision model params: --random-seed={}\".format(random_seed))\n",
    "    print(\"-----------------------------------------------------------------------------------\")\n",
    "    !python ../src/multimodal/train_vision.py \\\n",
    "        --model-version=pixels \\\n",
    "        --model-dir={model_dir}\\\n",
    "        --no-unique-dir \\\n",
    "        --random-seed={random_seed}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedforward Neural Network (FFNN) Softmax Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for random_seed in random_seeds:\n",
    "    model_dir = \"./models/vision/ffnn_softmax/batch_size={}/random_seed={}\".format(ffnn_batch_size,\n",
    "                                                                                   random_seed)\n",
    "    print(\"-----------------------------------------------------------------------------------\")\n",
    "    print(\"Training FFNN classifier vision model: --random-seed={}\".format(random_seed))\n",
    "    print(\"-----------------------------------------------------------------------------------\")\n",
    "    # Save base params file in model directory\n",
    "    !python ../src/multimodal/train_vision.py \\\n",
    "        --model-version=feedforward_softmax \\\n",
    "        --model-dir={model_dir} \\\n",
    "        --no-unique-dir \\\n",
    "        --save-base-params\n",
    "    # Load model params from file\n",
    "    with open(os.path.join(model_dir, 'model_params.json'), 'r') as fp:\n",
    "        model_params = json.load(fp)\n",
    "    # Set new model params\n",
    "    model_params['n_hidden_units'] = ffnn_n_hidden_units\n",
    "    model_params['n_linear_units'] = None  # automatically adds output logits\n",
    "    model_params['dropout_keep_prob'] = dropout_keep_prob\n",
    "    model_params['hidden_batch_norm'] = hidden_batch_norm\n",
    "    # Save updated model params to file\n",
    "    with open(os.path.join(model_dir, 'model_params.json'), 'w') as fp:\n",
    "        json.dump(model_params, fp, indent=4)\n",
    "    # Train ffnn softmax model on updated model params\n",
    "    !python ../src/multimodal/train_vision.py \\\n",
    "        --model-version=feedforward_softmax \\\n",
    "        --model-dir={model_dir} \\\n",
    "        --no-unique-dir \\\n",
    "        --n-max-epochs=100 \\\n",
    "        --random-seed={random_seed} \\\n",
    "        -lr={learn_rate} \\\n",
    "        -dr={decay_rate} \\\n",
    "        --batch-size={ffnn_batch_size} \\\n",
    "        --k-shot=1 \\\n",
    "        --l-way={l_way_validation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network (CNN) Softmax Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for random_seed in random_seeds:\n",
    "    model_dir = \"./models/vision/cnn_softmax/batch_size={}/random_seed={}\".format(cnn_batch_size,\n",
    "                                                                                  random_seed))\n",
    "    print(\"-----------------------------------------------------------------------------------\")\n",
    "    print(\"Training CNN classifier vision model: --random-seed={}\".format(random_seed))\n",
    "    print(\"-----------------------------------------------------------------------------------\")\n",
    "    # Save base params file in model directory\n",
    "    !python ../src/multimodal/train_vision.py \\\n",
    "        --model-version=convolutional_softmax \\\n",
    "        --model-dir={model_dir} \\\n",
    "        --no-unique-dir \\\n",
    "        --save-base-params\n",
    "    # Load model params from file\n",
    "    with open(os.path.join(model_dir, 'model_params.json'), 'r') as fp:\n",
    "        model_params = json.load(fp)\n",
    "    # Set new model params\n",
    "    model_params['kernel_sizes'] = kernel_sizes\n",
    "    model_params['pool_sizes'] = pool_sizes\n",
    "    model_params['n_filters'] = n_filters\n",
    "    model_params['n_hidden_units'] = n_hidden_units\n",
    "    model_params['n_linear_units'] = None  # automatically adds output logits\n",
    "    model_params['dropout_keep_prob'] = dropout_keep_prob\n",
    "    model_params['dropout_channels'] = dropout_channels\n",
    "    model_params['conv_batch_norm'] = conv_batch_norm\n",
    "    model_params['hidden_batch_norm'] = hidden_batch_norm\n",
    "    # Save updated model params to file\n",
    "    with open(os.path.join(model_dir, 'model_params.json'), 'w') as fp:\n",
    "        json.dump(model_params, fp, indent=4)\n",
    "    # Train conv softmax model on updated model params\n",
    "    !python ../src/multimodal/train_vision.py \\\n",
    "        --model-version=convolutional_softmax \\\n",
    "        --model-dir={model_dir} \\\n",
    "        --no-unique-dir \\\n",
    "        --n-max-epochs=100 \\\n",
    "        --random-seed={random_seed} \\\n",
    "        -lr={learn_rate} \\\n",
    "        -dr={decay_rate} \\\n",
    "        --batch-size={cnn_batch_size} \\\n",
    "        --k-shot=1 \\\n",
    "        --l-way={l_way_validation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Siamese CNN (offline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust k_batch and p_batch since Siamese offline variant hits memory limit\n",
    "# on a single Titan Xp GPU with larger batch sizes ...\n",
    "p_batch_adjust = 32  # wide spread of concepts per batch\n",
    "k_batch_adjust = 2   # few concept examples per batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for random_seed in random_seeds:\n",
    "    model_dir = \"./models/vision/siamese_offline/n_train={}/random_seed={}\".format(offline_n_train_episodes,\n",
    "                                                                                   random_seed)\n",
    "    print(\"-----------------------------------------------------------------------------------\")\n",
    "    print(\"Training Siamese CNN (offline) vision model: --random-seed={}\".format(random_seed))\n",
    "    print(\"-----------------------------------------------------------------------------------\")\n",
    "    # Save base params file in model directory\n",
    "    !python ../src/multimodal/train_vision.py \\\n",
    "        --model-version=siamese_triplet \\\n",
    "        --model-dir={model_dir} \\\n",
    "        --no-unique-dir \\\n",
    "        --save-base-params\n",
    "    # Load model params from file\n",
    "    with open(os.path.join(model_dir, 'model_params.json'), 'r') as fp:\n",
    "        model_params = json.load(fp)\n",
    "    # Set new model params\n",
    "    model_params['kernel_sizes'] = kernel_sizes\n",
    "    model_params['pool_sizes'] = pool_sizes\n",
    "    model_params['n_filters'] = n_filters\n",
    "    model_params['n_hidden_units'] = n_hidden_units\n",
    "    model_params['n_linear_units'] = n_linear_units\n",
    "    model_params['dropout_keep_prob'] = dropout_keep_prob\n",
    "    model_params['dropout_channels'] = dropout_channels\n",
    "    model_params['conv_batch_norm'] = conv_batch_norm\n",
    "    model_params['hidden_batch_norm'] = hidden_batch_norm\n",
    "    # Save updated model params to file\n",
    "    with open(os.path.join(model_dir, 'model_params.json'), 'w') as fp:\n",
    "        json.dump(model_params, fp, indent=4)\n",
    "    # Train siamese online vision model on updated model params\n",
    "    !python ../src/multimodal/train_vision.py \\\n",
    "        --model-version=siamese_triplet \\\n",
    "        --model-dir={model_dir} \\\n",
    "        --no-unique-dir \\\n",
    "        --n-max-epochs=100 \\\n",
    "        --random-seed={random_seed} \\\n",
    "        --n-train-episodes={offline_n_train_episodes} \\\n",
    "        -lr={learn_rate} \\\n",
    "        -dr={decay_rate} \\\n",
    "        --balanced-batching \\\n",
    "        --p-batch={p_batch_adjust} \\\n",
    "        --k-batch={k_batch_adjust} \\\n",
    "        --k-shot=1 \\\n",
    "        --l-way={l_way_validation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Siamese CNN (online)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for random_seed in random_seeds:\n",
    "    model_dir = \"./models/vision/siamese_online/n_train={}/random_seed={}\".format(online_n_train_episodes,\n",
    "                                                                                  random_seed)\n",
    "    print(\"-----------------------------------------------------------------------------------\")\n",
    "    print(\"Training Siamese CNN (online) vision model: --random-seed={}\".format(random_seed))\n",
    "    print(\"-----------------------------------------------------------------------------------\")\n",
    "    # Save base params file in model directory\n",
    "    !python ../src/multimodal/train_vision.py \\\n",
    "        --model-version=siamese_triplet_online \\\n",
    "        --model-dir={model_dir} \\\n",
    "        --no-unique-dir \\\n",
    "        --save-base-params\n",
    "    # Load model params from file\n",
    "    with open(os.path.join(model_dir, 'model_params.json'), 'r') as fp:\n",
    "        model_params = json.load(fp)\n",
    "    # Set new model params\n",
    "    model_params['kernel_sizes'] = kernel_sizes\n",
    "    model_params['pool_sizes'] = pool_sizes\n",
    "    model_params['n_filters'] = n_filters\n",
    "    model_params['n_hidden_units'] = n_hidden_units\n",
    "    model_params['n_linear_units'] = n_linear_units\n",
    "    model_params['dropout_keep_prob'] = dropout_keep_prob\n",
    "    model_params['dropout_channels'] = dropout_channels\n",
    "    model_params['conv_batch_norm'] = conv_batch_norm\n",
    "    model_params['hidden_batch_norm'] = hidden_batch_norm\n",
    "    # Save updated model params to file\n",
    "    with open(os.path.join(model_dir, 'model_params.json'), 'w') as fp:\n",
    "        json.dump(model_params, fp, indent=4)\n",
    "    # Train siamese online vision model on updated model params\n",
    "    !python ../src/multimodal/train_vision.py \\\n",
    "        --model-version=siamese_triplet_online \\\n",
    "        --model-dir={model_dir} \\\n",
    "        --no-unique-dir \\\n",
    "        --n-max-epochs=100 \\\n",
    "        --n-train-episodes={online_n_train_episodes} \\\n",
    "        --random-seed={random_seed} \\\n",
    "        -lr={learn_rate} \\\n",
    "        -dr={decay_rate} \\\n",
    "        --balanced-batching \\\n",
    "        --p-batch={p_batch} \\\n",
    "        --k-batch={k_batch} \\\n",
    "        --k-shot=1 \\\n",
    "        --l-way={l_way_validation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. One-shot vision testing\n",
    "<a id='vision_test'></a>\n",
    "\n",
    "We now test the trained unimodal vision models on two tasks on the [MNIST handwritten digit dataset](http://yann.lecun.com/exdb/mnist/):\n",
    "\n",
    "1. One-shot 10-way handwritten digit image classification\n",
    "2. Five-shot 10-way handwritten digit image classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pixel Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. One-shot 10-way handwritten digit image classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./results/vision/pixels/1_shot\"\n",
    "for random_seed in random_seeds:\n",
    "    model_dir = \"./models/vision/pixels/random_seed={}\".format(random_seed)\n",
    "    out_dir = os.path.join(output_dir, 'random_seed={}'.format(random_seed))\n",
    "    test_vision_k_shot(model_dir, out_dir, random_seed, k_shot=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Five-shot 10-way handwritten digit image classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./results/vision/pixels/5_shot\"\n",
    "for random_seed in random_seeds:\n",
    "    model_dir = \"./models/vision/pixels/random_seed={}\".format(random_seed)\n",
    "    out_dir = os.path.join(output_dir, 'random_seed={}'.format(random_seed))\n",
    "    test_vision_k_shot(model_dir, out_dir, random_seed, k_shot=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedforward Neural Network (FFNN) Softmax Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. One-shot 10-way handwritten digit image classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./results/vision/ffnn_softmax/1_shot/batch_size={}\".format(ffnn_batch_size)\n",
    "for random_seed in random_seeds:\n",
    "    model_dir = \"./models/vision/ffnn_softmax/batch_size={}/random_seed={}\".format(ffnn_batch_size,\n",
    "                                                                                   random_seed)\n",
    "    out_dir = os.path.join(output_dir, 'random_seed={}'.format(random_seed))\n",
    "    test_vision_k_shot(model_dir, out_dir, random_seed, k_shot=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Five-shot 10-way handwritten digit image classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./results/vision/ffnn_softmax/5_shot/batch_size={}\".format(ffnn_batch_size)\n",
    "for random_seed in random_seeds:\n",
    "    model_dir = \"./models/vision/ffnn_softmax/batch_size={}/random_seed={}\".format(ffnn_batch_size,\n",
    "                                                                                   random_seed)\n",
    "    out_dir = os.path.join(output_dir, 'random_seed={}'.format(random_seed))\n",
    "    test_vision_k_shot(model_dir, out_dir, random_seed, k_shot=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network (CNN) Softmax Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. One-shot 10-way handwritten digit image classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./results/vision/cnn_softmax/1_shot/batch_size={}\".format(cnn_batch_size)\n",
    "for random_seed in random_seeds:\n",
    "    model_dir = \"./models/vision/cnn_softmax/batch_size={}/random_seed={}\".format(cnn_batch_size,\n",
    "                                                                                   random_seed)\n",
    "    out_dir = os.path.join(output_dir, 'random_seed={}'.format(random_seed))\n",
    "    test_vision_k_shot(model_dir, out_dir, random_seed, k_shot=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Five-shot 10-way handwritten digit image classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./results/vision/cnn_softmax/5_shot/batch_size={}\".format(cnn_batch_size)\n",
    "for random_seed in random_seeds:\n",
    "    model_dir = \"./models/vision/cnn_softmax/batch_size={}/random_seed={}\".format(cnn_batch_size,\n",
    "                                                                                   random_seed)\n",
    "    out_dir = os.path.join(output_dir, 'random_seed={}'.format(random_seed))\n",
    "    test_vision_k_shot(model_dir, out_dir, random_seed, k_shot=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Siamese CNN (offline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. One-shot 10-way handwritten digit image classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./results/vision/siamese_offline/1_shot/n_train={}\".format(offline_n_train_episodes)\n",
    "for random_seed in random_seeds:\n",
    "    model_dir = \"./models/vision/siamese_offline/n_train={}/random_seed={}\".format(offline_n_train_episodes,\n",
    "                                                                                   random_seed)\n",
    "    out_dir = os.path.join(output_dir, 'random_seed={}'.format(random_seed))\n",
    "    test_vision_k_shot(model_dir, out_dir, random_seed, k_shot=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Five-shot 10-way handwritten digit image classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./results/vision/siamese_offline/5_shot/n_train={}\".format(offline_n_train_episodes)\n",
    "for random_seed in random_seeds:\n",
    "    model_dir = \"./models/vision/siamese_offline/n_train={}/random_seed={}\".format(offline_n_train_episodes,\n",
    "                                                                                   random_seed)\n",
    "    out_dir = os.path.join(output_dir, 'random_seed={}'.format(random_seed))\n",
    "    test_vision_k_shot(model_dir, out_dir, random_seed, k_shot=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Siamese CNN (online)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. One-shot 10-way handwritten digit image classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./results/vision/siamese_online/1_shot/n_train={}\".format(online_n_train_episodes)\n",
    "for random_seed in random_seeds:\n",
    "    model_dir = \"./models/vision/siamese_online/n_train={}/random_seed={}\".format(online_n_train_episodes,\n",
    "                                                                                   random_seed)\n",
    "    out_dir = os.path.join(output_dir, 'random_seed={}'.format(random_seed))\n",
    "    test_vision_k_shot(model_dir, out_dir, random_seed, k_shot=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Five-shot 10-way handwritten digit image classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./results/vision/siamese_online/5_shot/n_train={}\".format(online_n_train_episodes)\n",
    "for random_seed in random_seeds:\n",
    "    model_dir = \"./models/vision/siamese_online/n_train={}/random_seed={}\".format(online_n_train_episodes,\n",
    "                                                                                   random_seed)\n",
    "    out_dir = os.path.join(output_dir, 'random_seed={}'.format(random_seed))\n",
    "    test_vision_k_shot(model_dir, out_dir, random_seed, k_shot=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Summaries\n",
    "<a id='vision_summ'></a>\n",
    "\n",
    "This section presents summaries on the training and one-shot testing of the unimodal vision models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pixel Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. One-shot 10-way handwritten digit image classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir = './results/vision/pixels/1_shot'\n",
    "summarise_tests(result_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Five-shot 10-way handwritten digit image classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir = './results/vision/pixels/5_shot'\n",
    "summarise_tests(result_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedforward Neural Network (FFNN) Softmax Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir = \"./models/vision/ffnn_softmax/batch_size={}\".format(ffnn_batch_size)\n",
    "summarise_tests(result_dir, result_file='train_result.txt', result_file='train_result.txt')\n",
    "summarise_train_time(result_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. One-shot 10-way handwritten digit image classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir = \"./results/vision/ffnn_softmax/1_shot/batch_size={}\".format(ffnn_batch_size)\n",
    "summarise_tests(result_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Five-shot 10-way handwritten digit image classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir = \"./results/vision/ffnn_softmax/5_shot/batch_size={}\".format(ffnn_batch_size)\n",
    "summarise_tests(result_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network (CNN) Softmax Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir = \"./models/vision/cnn_softmax/batch_size={}\".format(cnn_batch_size)\n",
    "summarise_tests(result_dir, result_file='train_result.txt', result_file='train_result.txt')\n",
    "summarise_train_time(result_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. One-shot 10-way handwritten digit image classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir = \"./results/vision/cnn_softmax/1_shot/batch_size={}\".format(cnn_batch_size)\n",
    "summarise_tests(result_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Five-shot 10-way handwritten digit image classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir = \"./results/vision/cnn_softmax/5_shot/batch_size={}\".format(cnn_batch_size)\n",
    "summarise_tests(result_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Siamese CNN (offline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir = \"./models/vision/siamese_offline/n_train={}\".format(offline_n_train_episodes)\n",
    "summarise_tests(result_dir, result_file='train_result.txt')\n",
    "summarise_train_time(result_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. One-shot 10-way handwritten digit image classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir = \"./results/vision/siamese_offline/1_shot/n_train={}\".format(offline_n_train_episodes)\n",
    "summarise_tests(result_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Five-shot 10-way handwritten digit image classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir = \"./results/vision/siamese_offline/5_shot/n_train={}\".format(offline_n_train_episodes)\n",
    "summarise_tests(result_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Siamese CNN (online)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir = \"./models/vision/siamese_online/n_train={}\".format(online_n_train_episodes)\n",
    "summarise_tests(result_dir, result_file='train_result.txt')\n",
    "summarise_train_time(result_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. One-shot 10-way handwritten digit image classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir = \"./results/vision/siamese_online/1_shot/n_train={}\".format(online_n_train_episodes)\n",
    "summarise_tests(result_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Five-shot 10-way handwritten digit image classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir = \"./results/vision/siamese_online/5_shot/n_train={}\".format(online_n_train_episodes)\n",
    "summarise_tests(result_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
