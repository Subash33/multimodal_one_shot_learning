{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing of multimodal speech-vision models\n",
    "\n",
    "**Author:** Ryan Eloff<br>\n",
    "**Contact:** ryan.peter.eloff@gmail.com<br>\n",
    "**Date:** October 2018\n",
    "\n",
    "Experiments notebook 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "*Multimodal one-shot learning* is the problem of learning novel concepts from only *one or a few* examples of features in multiple modalities, with the only supervisory signal being that these features co-occur. \n",
    "Here we specifically consider multimodal one-shot learning on a dataset of isolated spoken digits paired with images (although any paired sensory information may be used).\n",
    "\n",
    "We approach this problem by extending unimodal one-shot models to the multimodal case. Assuming that we have such models that can measure similarity within a modality (see [experiments notebook 1](https://github.com/rpeloff/multimodal-one-shot-learning/blob/master/experiments/nb1_unimodal_train_test.ipynb)), we can perform one-shot cross-modal matching by unimodal comparisons through the multimodal support set.\n",
    "\n",
    "This notebook demonstrates how to extend unimodal models to multimodal one-shot learning, and reproduces the one-shot cross-modal matching (of speech-image digits) results presented in [our paper](https://arxiv.org/abs/1811.03875): \n",
    "R. Eloff, H. A. Engelbrecht, H. Kamper, \"Multimodal One-Shot Learning of Speech and Images,\" 2018."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Navigation\n",
    "\n",
    "1. [Generate random model seeds](#seeds)<br>\n",
    "2. [Multimodal one-shot models](#multimodal)<br>\n",
    "    2.1. [Test parameters](#test_params)<br>\n",
    "    2.2. [One-shot cross-modal matching tests](#multimodal_test)<br>\n",
    "    2.3. [Summaries](#multimodal_summ)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "\n",
    "try:  # check that DTW has been compiled\n",
    "    from src.dtw.speech_dtw import _dtw\n",
    "except ImportError:\n",
    "    print(\"Building DTW Cython code ...\")\n",
    "    !make clean -C ../src/dtw\n",
    "    !make -C ../src/dtw\n",
    "    from src.dtw.speech_dtw import _dtw  # should no longer raise ImportError after building Cython DTW code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_multimodal_k_shot(speech_model_dir, vision_model_dir, out_dir, random_seed, k_shot):\n",
    "    print(\"--------------------------------------------------------------------------------\")\n",
    "    print(\"Testing multimodal model:\\n\\t--speech-model-dir={}\\n\\t--vision-model-dir={}\"\n",
    "          .format(speech_model_dir, vision_model_dir))\n",
    "    print(\"--------------------------------------------------------------------------------\")\n",
    "    !python ../src/multimodal/test_multimodal.py \\\n",
    "        --speech-data-dir=../kaldi_features/tidigits \\\n",
    "        --speech-model-dir={speech_model_dir} \\\n",
    "        --vision-model-dir={vision_model_dir} \\\n",
    "        --output-dir={out_dir} \\\n",
    "        --random-seed={random_seed} \\\n",
    "        --zeros-different \\\n",
    "        --n-queries=10 \\\n",
    "        --n-test-episodes=400 \\\n",
    "        --k-shot={k_shot} \\\n",
    "        --l-way=11\n",
    "\n",
    "\n",
    "def test_speaker_invariance(speech_model_dir, vision_model_dir, out_dir, random_seed):\n",
    "    print(\"--------------------------------------------------------------------------------\")\n",
    "    print(\"Testing multimodal model:\\n\\t--speech-model-dir={}\\n\\t--vision-model-dir={}\"\n",
    "          .format(speech_model_dir, vision_model_dir))\n",
    "    print(\"--------------------------------------------------------------------------------\")\n",
    "    !python ../src/multimodal/test_multimodal.py \\\n",
    "        --speech-data-dir=../kaldi_features/tidigits \\\n",
    "        --speech-model-dir={speech_model_dir} \\\n",
    "        --vision-model-dir={vision_model_dir} \\\n",
    "        --output-dir={out_dir} \\\n",
    "        --random-seed={random_seed} \\\n",
    "        --zeros-different \\\n",
    "        --originator-type='difficult' \\\n",
    "        --n-queries=1 \\\n",
    "        --n-test-episodes=4000 \\\n",
    "        --k-shot=1 \\\n",
    "        --l-way=11\n",
    "\n",
    "\n",
    "def summarise_tests(result_dir, result_file='test_result.txt', speaker_invariance=False):\n",
    "    overall_results = []\n",
    "    easy_overall_results = []\n",
    "    dist_overall_results = []\n",
    "    for root, subdirs, files in os.walk(result_dir):\n",
    "        subdirs.sort()\n",
    "        for dirname in subdirs:\n",
    "            res_file = os.path.join(root, dirname, result_file)\n",
    "            if os.path.isfile(res_file):\n",
    "                print(\"--------------------------------------------------------------------------------\")\n",
    "                print(\"Model summary: directory={}\".format(os.path.join(root, dirname)))\n",
    "                print(\"--------------------------------------------------------------------------------\")\n",
    "                with open(res_file, 'r') as fp:\n",
    "                    results = fp.read()\n",
    "                print('\\tResults: {}'.format(results))\n",
    "                overall_results.append(float(results.split('\\n')[0].split('accuracy: ')[1]))\n",
    "                if speaker_invariance:\n",
    "                    invariance_results = results.split('\\n')[1].strip().split('\\t')\n",
    "                    easy_overall_results.append(float(invariance_results[0].split('accuracy: ')[1]))\n",
    "                    dist_overall_results.append(float(invariance_results[1].split('accuracy: ')[1]))\n",
    "    conf_interval_95 = 1.96 * np.std(overall_results) / np.sqrt(len(overall_results))\n",
    "    easy_conf_interval_95 = 1.96 * np.std(easy_overall_results) / np.sqrt(len(easy_overall_results))\n",
    "    dist_conf_interval_95 = 1.96 * np.std(dist_overall_results) / np.sqrt(len(dist_overall_results))\n",
    "    print(\"================================================================================\")\n",
    "    print(\"OVERALL: AVERAGE ACCURACY: {:.4f} % +- {:.4f} (total tests: {})\"\n",
    "          .format(np.mean(overall_results)*100, conf_interval_95*100, len(overall_results)))\n",
    "    if speaker_invariance:\n",
    "        print(\"\\tAVERAGE EASY SPEAKER ACCURACY: {:.4f} % +- {:.4f} (total tests: {})\"\n",
    "              .format(np.mean(easy_overall_results)*100, easy_conf_interval_95*100, len(easy_overall_results)))\n",
    "        print(\"\\tAVERAGE DISTRACTOR SPEAKER ACCURACY: {:.4f} % +- {:.4f} (total tests: {})\"\n",
    "              .format(np.mean(dist_overall_results)*100, dist_conf_interval_95*100, len(dist_overall_results)))\n",
    "    print(\"--------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate random model seeds\n",
    "<a id='seeds'></a>\n",
    "\n",
    "We average results over 10 models trained with different seeds so that we can report average accuracies with 95% confidence intervals.\n",
    "\n",
    "These seeds are generated as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "random_seeds = np.random.randint(1000, size=10)\n",
    "print(\"Random seeds:\", random_seeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Multimodal one-shot models\n",
    "<a id='multimodal'></a>\n",
    "\n",
    "The multimodal one-shot models that we present here are a combination of unimodal one-shot speech and vision models which are previosuly trained on background data that does not overlap with the multimodal one-shot task.\n",
    "These models require no further training, and we can directly perform one-shot cross-modal matching by unimodal comparisons through the multimodal support set.\n",
    "\n",
    "We specifically investigate Siamese neural networks trained for one-shot speech or image classification,\n",
    "and compare to directly matching images (pixels) and extracted speech features (dynamic time warping), as well as to transfer learning with neural network classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Test parameters\n",
    "<a id='test_params'></a>\n",
    "\n",
    "The following parameters were used to produce the multimodal one-shot learning results in the paper (only used for selecting correct models for testing):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DTW + pixels\n",
    "dtw_feats_type = 'mfcc'\n",
    "# FFNN classifier\n",
    "ffnn_batch_size = 200  # same for both modalities\n",
    "# CNN classifier\n",
    "cnn_batch_size = 200  # same for both modalities\n",
    "# Siamese CNN (offline)\n",
    "speech_offline_n_train_episodes = 200\n",
    "vision_offline_n_train_episodes = 600\n",
    "# Siamese CNN (online)\n",
    "speech_online_n_train_episodes = 50\n",
    "vision_online_n_train_episodes = 150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. One-shot cross-modal matching tests\n",
    "<a id='multimodal_test'></a>\n",
    "\n",
    "We test the trained multimodal speech-vision models on three tasks, where speech-image pairs are randomly selected from the [TIDigits speech corpus](https://catalog.ldc.upenn.edu/LDC93S10) and [MNIST handwritten digit dataset](http://yann.lecun.com/exdb/mnist/):\n",
    "\n",
    "1. One-shot 11-way cross-modal speech-image digit matching\n",
    "2. Five-shot 11-way cross-modal speech-image digit matching\n",
    "3. Speaker invariance for one-shot 11-way cross-modal speech-image digit matching in the presence of query speaker distractors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic Time Warping (DTW) for Speech + Pixel Matching for Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. One-shot 11-way cross-modal speech-image digit matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./results/multimodal/dtw_pixels/1_shot/{}\".format(dtw_feats_type)\n",
    "for random_seed in random_seeds:\n",
    "    speech_model_dir = \"./models/speech/dtw/{}/random_seed={}\".format(dtw_feats_type,\n",
    "                                                                      random_seed)\n",
    "    vision_model_dir = \"./models/vision/pixels/random_seed={}\".format(random_seed)\n",
    "    out_dir = os.path.join(output_dir, 'random_seed={}'.format(random_seed))\n",
    "    test_multimodal_k_shot(speech_model_dir, vision_model_dir, out_dir, random_seed, k_shot=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Five-shot 11-way cross-modal speech-image digit matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./results/multimodal/dtw_pixels/5_shot/{}\".format(dtw_feats_type)\n",
    "for random_seed in random_seeds:\n",
    "    speech_model_dir = \"./models/speech/dtw/{}/random_seed={}\".format(dtw_feats_type,\n",
    "                                                                      random_seed)\n",
    "    vision_model_dir = \"./models/vision/pixels/random_seed={}\".format(random_seed)\n",
    "    out_dir = os.path.join(output_dir, 'random_seed={}'.format(random_seed))\n",
    "    test_multimodal_k_shot(speech_model_dir, vision_model_dir, out_dir, random_seed, k_shot=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Speaker invariance for one-shot 11-way cross-modal speech-image digit matching in the presence of query speaker distractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./results/multimodal/dtw_pixels/speaker_invariance/{}\".format(dtw_feats_type)\n",
    "for random_seed in random_seeds:\n",
    "    speech_model_dir = \"./models/speech/dtw/{}/random_seed={}\".format(dtw_feats_type,\n",
    "                                                                      random_seed)\n",
    "    vision_model_dir = \"./models/vision/pixels/random_seed={}\".format(random_seed)\n",
    "    out_dir = os.path.join(output_dir, 'random_seed={}'.format(random_seed))\n",
    "    test_speaker_invariance(speech_model_dir, vision_model_dir, out_dir, random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedforward Neural Network (FFNN) Softmax Classifiers for Speech and Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. One-shot 11-way cross-modal speech-image digit matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./results/multimodal/ffnn_softmax/1_shot/batch_size={}\".format(ffnn_batch_size)\n",
    "for random_seed in random_seeds:\n",
    "    speech_model_dir = \"./models/speech/ffnn_softmax/batch_size={}/random_seed={}\".format(ffnn_batch_size,\n",
    "                                                                                          random_seed)\n",
    "    vision_model_dir = \"./models/vision/ffnn_softmax/batch_size={}/random_seed={}\".format(ffnn_batch_size,\n",
    "                                                                                          random_seed)\n",
    "    out_dir = os.path.join(output_dir, 'random_seed={}'.format(random_seed))\n",
    "    test_multimodal_k_shot(speech_model_dir, vision_model_dir, out_dir, random_seed, k_shot=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Five-shot 11-way cross-modal speech-image digit matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./results/multimodal/ffnn_softmax/5_shot/batch_size={}\".format(ffnn_batch_size)\n",
    "for random_seed in random_seeds:\n",
    "    speech_model_dir = \"./models/speech/ffnn_softmax/batch_size={}/random_seed={}\".format(ffnn_batch_size,\n",
    "                                                                                          random_seed)\n",
    "    vision_model_dir = \"./models/vision/ffnn_softmax/batch_size={}/random_seed={}\".format(ffnn_batch_size,\n",
    "                                                                                          random_seed)\n",
    "    out_dir = os.path.join(output_dir, 'random_seed={}'.format(random_seed))\n",
    "    test_multimodal_k_shot(speech_model_dir, vision_model_dir, out_dir, random_seed, k_shot=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Speaker invariance for one-shot 11-way cross-modal speech-image digit matching in the presence of query speaker distractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./results/multimodal/ffnn_softmax/speaker_invariance/batch_size={}\".format(ffnn_batch_size)\n",
    "for random_seed in random_seeds:\n",
    "    speech_model_dir = \"./models/speech/ffnn_softmax/batch_size={}/random_seed={}\".format(ffnn_batch_size,\n",
    "                                                                                          random_seed)\n",
    "    vision_model_dir = \"./models/vision/ffnn_softmax/batch_size={}/random_seed={}\".format(ffnn_batch_size,\n",
    "                                                                                          random_seed)\n",
    "    out_dir = os.path.join(output_dir, 'random_seed={}'.format(random_seed))\n",
    "    test_speaker_invariance(speech_model_dir, vision_model_dir, out_dir, random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network (CNN) Softmax Classifiers for Speech and Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. One-shot 11-way cross-modal speech-image digit matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./results/multimodal/cnn_softmax/1_shot/batch_size={}\".format(cnn_batch_size)\n",
    "for random_seed in random_seeds:\n",
    "    speech_model_dir = \"./models/speech/cnn_softmax/batch_size={}/random_seed={}\".format(cnn_batch_size,\n",
    "                                                                                         random_seed)\n",
    "    vision_model_dir = \"./models/vision/cnn_softmax/batch_size={}/random_seed={}\".format(cnn_batch_size,\n",
    "                                                                                         random_seed)\n",
    "    out_dir = os.path.join(output_dir, 'random_seed={}'.format(random_seed))\n",
    "    test_multimodal_k_shot(speech_model_dir, vision_model_dir, out_dir, random_seed, k_shot=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Five-shot 11-way cross-modal speech-image digit matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./results/multimodal/cnn_softmax/5_shot/batch_size={}\".format(cnn_batch_size)\n",
    "for random_seed in random_seeds:\n",
    "    speech_model_dir = \"./models/speech/cnn_softmax/batch_size={}/random_seed={}\".format(cnn_batch_size,\n",
    "                                                                                         random_seed)\n",
    "    vision_model_dir = \"./models/vision/cnn_softmax/batch_size={}/random_seed={}\".format(cnn_batch_size,\n",
    "                                                                                         random_seed)\n",
    "    out_dir = os.path.join(output_dir, 'random_seed={}'.format(random_seed))\n",
    "    test_multimodal_k_shot(speech_model_dir, vision_model_dir, out_dir, random_seed, k_shot=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Speaker invariance for one-shot 11-way cross-modal speech-image digit matching in the presence of query speaker distractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./results/multimodal/cnn_softmax/speaker_invariance/batch_size={}\".format(cnn_batch_size)\n",
    "for random_seed in random_seeds:\n",
    "    speech_model_dir = \"./models/speech/cnn_softmax/batch_size={}/random_seed={}\".format(cnn_batch_size,\n",
    "                                                                                         random_seed)\n",
    "    vision_model_dir = \"./models/vision/cnn_softmax/batch_size={}/random_seed={}\".format(cnn_batch_size,\n",
    "                                                                                         random_seed)\n",
    "    out_dir = os.path.join(output_dir, 'random_seed={}'.format(random_seed))\n",
    "    test_speaker_invariance(speech_model_dir, vision_model_dir, out_dir, random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Siamese CNN (offline) Comparators for Speech and Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. One-shot 11-way cross-modal speech-image digit matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./results/multimodal/siamese_offline/1_shot/n_train_speech={}_vision={}\".format(\n",
    "    speech_offline_n_train_episodes, vision_offline_n_train_episodes)\n",
    "for random_seed in random_seeds:\n",
    "    speech_model_dir = \"./models/speech/siamese_offline/n_train={}/random_seed={}\".format(\n",
    "        speech_offline_n_train_episodes, random_seed)\n",
    "    vision_model_dir = \"./models/vision/siamese_offline/n_train={}/random_seed={}\".format(\n",
    "        vision_offline_n_train_episodes, random_seed)\n",
    "    out_dir = os.path.join(output_dir, 'random_seed={}'.format(random_seed))\n",
    "    test_multimodal_k_shot(speech_model_dir, vision_model_dir, out_dir, random_seed, k_shot=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Five-shot 11-way cross-modal speech-image digit matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./results/multimodal/siamese_offline/5_shot/n_train_speech={}_vision={}\".format(\n",
    "    speech_offline_n_train_episodes, vision_offline_n_train_episodes)\n",
    "for random_seed in random_seeds:\n",
    "    speech_model_dir = \"./models/speech/siamese_offline/n_train={}/random_seed={}\".format(\n",
    "        speech_offline_n_train_episodes, random_seed)\n",
    "    vision_model_dir = \"./models/vision/siamese_offline/n_train={}/random_seed={}\".format(\n",
    "        vision_offline_n_train_episodes, random_seed)\n",
    "    out_dir = os.path.join(output_dir, 'random_seed={}'.format(random_seed))\n",
    "    test_multimodal_k_shot(speech_model_dir, vision_model_dir, out_dir, random_seed, k_shot=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Speaker invariance for one-shot 11-way cross-modal speech-image digit matching in the presence of query speaker distractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./results/multimodal/siamese_offline/speaker_invariance/n_train_speech={}_vision={}\".format(\n",
    "    speech_offline_n_train_episodes, vision_offline_n_train_episodes)\n",
    "for random_seed in random_seeds:\n",
    "    speech_model_dir = \"./models/speech/siamese_offline/n_train={}/random_seed={}\".format(\n",
    "        speech_offline_n_train_episodes, random_seed)\n",
    "    vision_model_dir = \"./models/vision/siamese_offline/n_train={}/random_seed={}\".format(\n",
    "        vision_offline_n_train_episodes, random_seed)\n",
    "    out_dir = os.path.join(output_dir, 'random_seed={}'.format(random_seed))\n",
    "    test_speaker_invariance(speech_model_dir, vision_model_dir, out_dir, random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Siamese CNN (online) Comparators for Speech and Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. One-shot 11-way cross-modal speech-image digit matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./results/multimodal/siamese_online/1_shot/n_train_speech={}_vision={}\".format(\n",
    "    speech_online_n_train_episodes, vision_online_n_train_episodes)\n",
    "for random_seed in random_seeds:\n",
    "    speech_model_dir = \"./models/speech/siamese_online/n_train={}/random_seed={}\".format(\n",
    "        speech_online_n_train_episodes, random_seed)\n",
    "    vision_model_dir = \"./models/vision/siamese_online/n_train={}/random_seed={}\".format(\n",
    "        vision_online_n_train_episodes, random_seed)\n",
    "    out_dir = os.path.join(output_dir, 'random_seed={}'.format(random_seed))\n",
    "    test_multimodal_k_shot(speech_model_dir, vision_model_dir, out_dir, random_seed, k_shot=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Five-shot 11-way cross-modal speech-image digit matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./results/multimodal/siamese_online/5_shot/n_train_speech={}_vision={}\".format(\n",
    "    speech_online_n_train_episodes, vision_online_n_train_episodes)\n",
    "for random_seed in random_seeds:\n",
    "    speech_model_dir = \"./models/speech/siamese_online/n_train={}/random_seed={}\".format(\n",
    "        speech_online_n_train_episodes, random_seed)\n",
    "    vision_model_dir = \"./models/vision/siamese_online/n_train={}/random_seed={}\".format(\n",
    "        vision_online_n_train_episodes, random_seed)\n",
    "    out_dir = os.path.join(output_dir, 'random_seed={}'.format(random_seed))\n",
    "    test_multimodal_k_shot(speech_model_dir, vision_model_dir, out_dir, random_seed, k_shot=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Speaker invariance for one-shot 11-way cross-modal speech-image digit matching in the presence of query speaker distractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./results/multimodal/siamese_online/speaker_invariance/n_train_speech={}_vision={}\".format(\n",
    "    speech_online_n_train_episodes, vision_online_n_train_episodes)\n",
    "for random_seed in random_seeds:\n",
    "    speech_model_dir = \"./models/speech/siamese_online/n_train={}/random_seed={}\".format(\n",
    "        speech_online_n_train_episodes, random_seed)\n",
    "    vision_model_dir = \"./models/vision/siamese_online/n_train={}/random_seed={}\".format(\n",
    "        vision_online_n_train_episodes, random_seed)\n",
    "    out_dir = os.path.join(output_dir, 'random_seed={}'.format(random_seed))\n",
    "    test_speaker_invariance(speech_model_dir, vision_model_dir, out_dir, random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Summaries\n",
    "<a id='multimodal_summ'></a>\n",
    "\n",
    "This section presents summaries on the one-shot testing of the multimodal models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic Time Warping (DTW) for Speech + Pixel Matching for Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. One-shot 11-way cross-modal speech-image digit matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir = \"./results/multimodal/dtw_pixels/1_shot/{}\".format(dtw_feats_type)\n",
    "summarise_tests(result_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Five-shot 11-way cross-modal speech-image digit matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir = \"./results/multimodal/dtw_pixels/5_shot/{}\".format(dtw_feats_type)\n",
    "summarise_tests(result_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Speaker invariance for one-shot 11-way cross-modal speech-image digit matching in the presence of query speaker distractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir = \"./results/multimodal/dtw_pixels/speaker_invariance/{}\".format(dtw_feats_type)\n",
    "summarise_tests(result_dir, speaker_invariance=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedforward Neural Network (FFNN) Softmax Classifiers for Speech and Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. One-shot 11-way cross-modal speech-image digit matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir = \"./results/multimodal/ffnn_softmax/1_shot/batch_size={}\".format(ffnn_batch_size)\n",
    "summarise_tests(result_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Five-shot 11-way cross-modal speech-image digit matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir = \"./results/multimodal/ffnn_softmax/5_shot/batch_size={}\".format(ffnn_batch_size)\n",
    "summarise_tests(result_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Speaker invariance for one-shot 11-way cross-modal speech-image digit matching in the presence of query speaker distractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir = \"./results/multimodal/ffnn_softmax/speaker_invariance/batch_size={}\".format(ffnn_batch_size)\n",
    "summarise_tests(result_dir, speaker_invariance=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network (CNN) Softmax Classifiers for Speech and Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. One-shot 11-way cross-modal speech-image digit matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir = \"./results/multimodal/cnn_softmax/1_shot/batch_size={}\".format(cnn_batch_size)\n",
    "summarise_tests(result_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Five-shot 11-way cross-modal speech-image digit matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir = \"./results/multimodal/cnn_softmax/5_shot/batch_size={}\".format(cnn_batch_size)\n",
    "summarise_tests(result_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Speaker invariance for one-shot 11-way cross-modal speech-image digit matching in the presence of query speaker distractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir = \"./results/multimodal/cnn_softmax/speaker_invariance/batch_size={}\".format(cnn_batch_size)\n",
    "summarise_tests(result_dir, speaker_invariance=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Siamese CNN (offline) Comparators for Speech and Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. One-shot 11-way cross-modal speech-image digit matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir = \"./results/multimodal/siamese_offline/1_shot/n_train_speech={}_vision={}\".format(\n",
    "    speech_offline_n_train_episodes, vision_offline_n_train_episodes)\n",
    "summarise_tests(result_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Five-shot 11-way cross-modal speech-image digit matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir = \"./results/multimodal/siamese_offline/5_shot/n_train_speech={}_vision={}\".format(\n",
    "    speech_offline_n_train_episodes, vision_offline_n_train_episodes)\n",
    "summarise_tests(result_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Speaker invariance for one-shot 11-way cross-modal speech-image digit matching in the presence of query speaker distractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir = \"./results/multimodal/siamese_offline/speaker_invariance/n_train_speech={}_vision={}\".format(\n",
    "    speech_offline_n_train_episodes, vision_offline_n_train_episodes)\n",
    "summarise_tests(result_dir, speaker_invariance=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Siamese CNN (online) Comparators for Speech and Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. One-shot 11-way cross-modal speech-image digit matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir = \"./results/multimodal/siamese_online/1_shot/n_train_speech={}_vision={}\".format(\n",
    "    speech_online_n_train_episodes, vision_online_n_train_episodes)\n",
    "summarise_tests(result_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Five-shot 11-way cross-modal speech-image digit matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir = \"./results/multimodal/siamese_online/5_shot/n_train_speech={}_vision={}\".format(\n",
    "    speech_online_n_train_episodes, vision_online_n_train_episodes)\n",
    "summarise_tests(result_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Speaker invariance for one-shot 11-way cross-modal speech-image digit matching in the presence of query speaker distractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir = \"./results/multimodal/siamese_online/speaker_invariance/n_train_speech={}_vision={}\".format(\n",
    "    speech_online_n_train_episodes, vision_online_n_train_episodes)\n",
    "summarise_tests(result_dir, speaker_invariance=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
